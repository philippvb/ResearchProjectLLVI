\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{pgfplots}
\usepackage{amsmath, enumerate, amssymb, multirow, fancyhdr, color, graphicx, lastpage, listings, tikz, pdflscape, subfigure, float, polynom, hyperref, tabularx, forloop, geometry, listings, fancybox, tikz, forest, tabstackengine, cancel}
\input kvmacros
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
\pagestyle {fancy}


\begin{document}


\title{Single Sample ELBO vs Laplace}
\maketitle
We want to show that the parameters of the gaussian of the weight distribution
when optimizing the ELBO is the same as standard optimization with a subsequent
laplace approximation. 

\section{General Objective function}
  The general form of the ELBO is:
  \begin{align*}
      \mathbb{E}_{q(\theta)}[\sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))}] - D_{KL}(q(\theta) \Vert p(\theta))\\
  \end{align*}
  if we have just one sample the expected log likelihood collapses to a standard likelihood
  \begin{align*}
    \sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))} - D_{KL}(q(\theta) \Vert p(\theta))\\
  \end{align*}
  where $\theta \sim q(\theta)$.\\
  If we redefine this loss as a probability distribution, more specific a Gibbs distribution, we have:
  \begin{align*}
    \prod_{i=1}^n p(y_i \vert f_{\theta}(x_i)) \cdot \exp(- D_{KL}(q(\theta) \Vert p(\theta)))\\
  \end{align*}
  so if we redefine the KL div as a prior over the weights, we have:
  \begin{align*}
    p(\theta \vert X,y) \propto \prod_{i=1}^n p(y_i \vert f_{\theta}(x_i)) \cdot p(\theta)\\
  \end{align*}
  This posterior is used for the Laplace approximation.

\section{Regression}

    
  \subsection{Closed form}
    In the case of Regression, we have the closed form likelihood:
          \begin{align*}
            \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A^{-1})}]
            &= -\frac{k}{2}\log{2\pi} - \frac{1}{2} \log(\vert A^{-1} \vert) + \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [(\phi_{X}\theta - y)^T A^{-1} (\phi_{X}\theta - y)]\\
            &= -\frac{k}{2}\log{2\pi} - \frac{1}{2} \log(\vert A^{-1} \vert) + Tr(A^{-1} \phi_X^T \Sigma \phi_X) + (\phi_X \mu - y)^T A^{-1} (\phi_X \mu -y)
          \end{align*}
  
    \subsubsection{Mean}
      If we take the derivative wrt $\mu$

      \begin{align}
        \nabla_{\mu} \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A^{-1})}]
        &=\nabla_{\mu} \langle \phi_{X} \mu - y, A^{-1} (\phi_{X} \mu -y) \rangle \label{eq: RegressionLoss}\\ 
        &= 2 \phi_X^T A (\phi_{X} \mu - y) \label{eq:ClosedForm_Mean}
      \end{align}

      If we take the standard MSE loss for Regression, we get the same derivative
      as in \ref{eq: RegressionLoss}, thus we get the same optimization result. To
      get to MAP estimation, we just impose the KL divergence as a prior over the
      weigths.
    
    \subsubsection{Variance}

      \begin{align}
        \nabla_\Sigma \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A^{-1})}]
        &= \nabla_\Sigma Tr(A^{-1} \phi_X^T \Sigma \phi_X)\\
        &= \phi_X^T \phi_X A^{-1^T}
      \end{align}

      For the laplace approximation, we need to compute:
      \begin{align}
        \nabla^2_{\mu} \log p(\theta \vert D)
        &\propto \nabla^2_{\mu} \langle \phi_X \mu - y, A^{-1} (\phi_X \mu -y) \rangle \\
        &= \nabla_{\mu} 2 \phi_X^T A^{-1} (\phi_X\mu - y)\\
        &= 2 \phi_X^T A^{-1} \phi_X \label{eq:Laplace_Covariance}
      \end{align}
      and then take the negative inverse, so we end up with:
      $\Sigma = (- 2 \phi_X^T A^{-1} \phi_X )^{-1}$

      The problem is now, that while the terms are the same (up to scaling), for the
      ELBO this is the derivative whereas for Laplace this is the approximation.
      possible further steps:
      \begin{itemize}
        \item extract term from KL-divergence so that we end up with with \ref{eq:Laplace_Covariance}, however then not consistent anymore with mean derivation.
      \end{itemize}

      \subsection{Single Sample}
        The likelihood for MC sampling with a single sample becomes:
        \begin{align*}
          \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A^{-1})}]
          &\approx -\frac{k}{2}\log{2\pi} - \frac{1}{2} \log(\vert A^{-1} \vert) + (\phi_{X} (\mu + L \epsilon) - y)^T A^{-1} (\phi_{X}(\mu + L \epsilon) - y)
        \end{align*}
        where $\epsilon \sim \mathcal{N}(0,1)$ and $L$ is the cholesky decomposition $L L^T = \Sigma$.

    \subsubsection{Mean}
      \begin{align}
        \nabla_{\mu} \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A^{-1})}]
        &\approx \nabla_{\mu} \langle \phi_{X} (\mu + L \epsilon) - y, A^{-1} (\phi_{X}(\mu + L \epsilon) - y) \rangle\\
        &= 2\phi_X^T A^{-1} (\phi_X \mu  - y) + 2 \phi_X^T A^{-1} \phi_X L \epsilon
      \end{align}

      Here the first term corresponds to the solution from
      \ref{eq:ClosedForm_Mean}, however we get an additional term. We could
      interpret this as performing noisy gradient descent since in expectation
      the noise will be 0. Then we also arrive at the same result as for the
      Laplace approximation.

    \subsubsection{Variance}
      \begin{align}
        \nabla_{\Sigma} \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A^{-1})}]
        &\approx \nabla_{\Sigma} \langle \phi_{X} (\mu + L \epsilon) - y, A^{-1} (\phi_{X}(\mu + L \epsilon) - y) \rangle\\
      \end{align}

      % Here we would need to take the derivative wrt a cholesky, complicated but doable for example https://mathoverflow.net/questions/150427/the-derivative-of-the-cholesky-factor



  \section{Results from Papers}
      Outline of proof:
      \begin{enumerate}
        \item We can show that Laplace is equal to the Bayesian Learning Rule
        \item The bayesian learning rule is equal to VI 
        \item Therefore Laplace is equal to VI
      \end{enumerate}

      We can see that even in equation 44 and 46 of the BLR paper: Here both are
      expressed in terms of the BLR, however we see that Laplace approximates
      the expectation whereas VI computes the actual one.

      In a more intuitive sense, we see that VI updates the Variance with a
      moving average of the Hessian, see equation 7 of paper `Noisy Natural
      Gradient as Variational Inference`. Since Laplace uses the Hessian as the
      approximation for the Variance, and if we assume to compute the Hessian
      online, we arrive at Laplace.

  \section{Outline proof with paper}
    Suppose we want to optimize a loss function $l(\theta)$ returning a
    probability distribution $q(\theta)$ over the parameters. The bayesian
    learning rule says that in this case, we should update the natural
    parameters $\lambda$ of $q$ with:
    \begin{equation}
      \lambda_{t+1} \leftarrow \lambda_{t} - \rho \tilde{\nabla}_\lambda E_{q_t}[l(\theta) - H(q_t)]
    \end{equation}
    where $H(q_t)$ is the entropy and $\tilde{\nabla}$ the Natural Gradient.
    Furthermore, this update is equal to optimizing Variational Inference on the
    inital problem.

    Now let $q$ be a Normal with unknown mean $m$ and precision $S$. It can be shown that
    in this case, the updates become:
    \begin{equation}
      m_{t+1} = m_t - \rho_t S^{-1} E_{q_t} [\nabla_\theta l(\theta)]
    \end{equation}
    and 
    \begin{equation}
      S_{t+1} = (1 - \rho_t) S_t + \rho_t E_{q_t}[\nabla^2_\theta l(\theta)]
    \end{equation}
    Here, we see that our Precision gets updated with the Hessian, which means
    we get the same result as if we would do an online computation of the
    Hessian during training in Laplace.

    We get similar result in the two other papers, see equation 7 in Noisy
    Natural Gradient and equation 7 and 8 in Fast and Scalable.

    

\end{document}

