{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "sys.path.append('/Users/philippvonbachmann/Documents/University/WiSe2122/ResearchProject/ResearchProjectLLVI/BasicExample')\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "from laplace import Laplace\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from src.weight_distribution.Full import FullCovariance\n",
    "from src.weight_distribution.Diagonal import Diagonal\n",
    "from src.network.Classification import LLVIClassification\n",
    "from src.network import PredictApprox, LikApprox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, out_dim=10, optimizer=optim.Adam, **optim_kwargs):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.fc2 = nn.Linear(120, 50)\n",
    "        self.fc3 = nn.Linear(50, out_dim, bias=False)\n",
    "        self.optimizer: optim = optimizer(self.parameters(), **optim_kwargs)\n",
    "        self.nonll = torch.sigmoid # nonlinear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.nonll(self.conv1(x)))\n",
    "        x = self.pool(self.nonll(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.nonll(self.fc1(x))\n",
    "        x = self.nonll(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "class VICNN(nn.Module):\n",
    "    def __init__(self, feature_dim=50, optimizer=optim.Adam, **optim_kwargs):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.fc2 = nn.Linear(120, feature_dim)\n",
    "        self.optimizer: optim = optimizer(self.parameters(), **optim_kwargs)\n",
    "        self.nonll = torch.sigmoid # nonlinear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.nonll(self.conv1(x)))\n",
    "        x = self.pool(self.nonll(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.nonll(self.fc1(x))\n",
    "        x = self.nonll(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/DeepLearning/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/runner/miniforge3/conda-bld/pytorch-recipe_1635217266490/work/torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "batch_size_train = 32\n",
    "batch_size_test = 60000\n",
    "filepath = \"/Users/philippvonbachmann/Documents/University/WiSe2122/ResearchProject/ResearchProjectLLVI/BasicExample/datasets/Classification\"\n",
    "# create dataset\n",
    "dataset = torchvision.datasets.MNIST(filepath, train=True, download=False,\n",
    "                            transform=torchvision.transforms.Compose([\n",
    "                              torchvision.transforms.ToTensor(),\n",
    "                              torchvision.transforms.Normalize(\n",
    "                                (0.1307,), (0.3081,))\n",
    "                            ]))\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size_test, shuffle=True)\n",
    "n_datapoints = batch_size_train * len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general parameters\n",
    "feature_dim = 50\n",
    "out_dim=10\n",
    "weight_decay = 1e-3\n",
    "lr = 1e-3\n",
    "base_train_epochs = 10\n",
    "\n",
    "# VI parameters\n",
    "tau = 1\n",
    "vi_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34: 100%|██████████| 10/10 [02:09<00:00, 12.94s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "laplace_model = CNN(weight_decay=weight_decay, lr=lr, out_dim=out_dim)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train\n",
    "pbar = tqdm(range(base_train_epochs))\n",
    "for i in pbar:\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        laplace_model.optimizer.zero_grad()\n",
    "        output = laplace_model(X_batch)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        laplace_model.optimizer.step()\n",
    "\n",
    "    pbar.set_description(f\"Loss: {round(float(torch.mean(loss)), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Laplace 0.9710000157356262\n"
     ]
    }
   ],
   "source": [
    "# define laplace\n",
    "la = Laplace(laplace_model, \"classification\",\n",
    "    subset_of_weights=\"last_layer\", hessian_structure=\"diag\",\n",
    "    prior_precision=weight_decay) # prior precision is set to wdecay\n",
    "la.fit(train_loader)\n",
    "for X_batch, y_batch in test_loader:\n",
    "    predictions = la(X_batch, link_approx='mc', n_samples=1000)\n",
    "    pred_test = torch.argmax(predictions, dim=1)\n",
    "    print(\"Accuracy with Laplace\", torch.mean((pred_test == y_batch).float()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with VI before Training 0.9726999998092651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prediction_loss:0.15 kl_loss:0.73: 100%|██████████| 10/10 [02:37<00:00, 15.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with VI after Training 0.972100019454956\n"
     ]
    }
   ],
   "source": [
    "dist = Diagonal(50, 10, lr=lr)\n",
    "dist.update_var(torch.reshape(la.posterior_variance, (50, 10)))\n",
    "dist.update_mean(torch.t(laplace_model.fc3.weight))\n",
    "\n",
    "# dist = FullCovariance(50, 10, lr=lr)\n",
    "# dist.update_cov(la.posterior_covariance)\n",
    "# dist.update_mean(torch.t(laplace_model.fc3.weight))\n",
    "\n",
    "\n",
    "vi_model = VICNN(weight_decay=weight_decay, lr=lr, feature_dim=feature_dim)\n",
    "with torch.no_grad():\n",
    "    vi_model.load_state_dict(laplace_model.state_dict(), strict=False)\n",
    "\n",
    "\n",
    "prior_log_var = math.log(1/(weight_decay * n_datapoints))\n",
    "prior_log_var = -7\n",
    "net = LLVIClassification(50, 10, vi_model, dist, prior_log_var=prior_log_var, optimizer_type=torch.optim.Adam,\n",
    "tau=tau, lr=lr)\n",
    "\n",
    "for X_batch, y_batch in test_loader:\n",
    "    predictions = net(X_batch, method=PredictApprox.MONTECARLO, samples=100)\n",
    "    pred_test = torch.argmax(predictions, dim=1)\n",
    "    print(\"Accuracy with VI before Training\", torch.mean((pred_test == y_batch).float()).item())\n",
    "\n",
    "net.train_model(train_loader, epochs=vi_train_epochs, n_datapoints=n_datapoints, samples=10, method=LikApprox.MONTECARLO)\n",
    "\n",
    "for X_batch, y_batch in test_loader:\n",
    "    predictions = net(X_batch, method=PredictApprox.MONTECARLO, samples=100)\n",
    "    pred_test = torch.argmax(predictions, dim=1)\n",
    "    print(\"Accuracy with VI after Training\", torch.mean((pred_test == y_batch).float()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "torchvision.datasets.MNIST(filepath, train=False, download=False,\n",
    "                            transform=torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.Normalize(\n",
    "                                (0.1307,), (0.3081,))\n",
    "                            ])),\n",
    "batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "ood_test_loader = torch.utils.data.DataLoader(\n",
    "torchvision.datasets.MNIST(filepath, train=False, download=True,\n",
    "                            transform=torchvision.transforms.Compose([\n",
    "                                torchvision.transforms.ToTensor(),\n",
    "                                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                torchvision.transforms.RandomHorizontalFlip(p=1)\n",
    "                            ])),\n",
    "batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_confidence(predict_fun, test_loader, ood_test_loader):\n",
    "    confidence_batch = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = predict_fun(data)\n",
    "            pred, _ = torch.max(output, dim=1) # confidence in choice\n",
    "            confidence_batch.append(torch.mean(pred))\n",
    "        print(f\"The mean confidence for in distribution data is: {sum(confidence_batch)/len(confidence_batch)}\")\n",
    "\n",
    "    ood_confidence_batch = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in ood_test_loader:\n",
    "            output = predict_fun(data)\n",
    "            pred, _ = torch.max(output, dim=1) # confidence in choice\n",
    "            ood_confidence_batch.append(torch.mean(pred))\n",
    "        print(f\"The mean confidence for out-of distribution data is: {sum(ood_confidence_batch)/len(ood_confidence_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_samples = 100\n",
    "la_predict_fun = lambda x: la(x, link_approx='mc', n_samples=predict_samples)\n",
    "vi_predict_fun = lambda x: net(x, method=PredictApprox.MONTECARLO, samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean confidence for in distribution data is: 0.9315642714500427\n",
      "The mean confidence for out-of distribution data is: 0.7018277049064636\n"
     ]
    }
   ],
   "source": [
    "test_confidence(la_predict_fun, test_loader, ood_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean confidence for in distribution data is: 0.9121099710464478\n",
      "The mean confidence for out-of distribution data is: 0.6925305724143982\n"
     ]
    }
   ],
   "source": [
    "test_confidence(vi_predict_fun, test_loader, ood_test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cabe5e2adc12bc55fec3b05872858fd36eca333f32d669b1230866c442273ae7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('DeepLearning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
