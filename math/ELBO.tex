\newcommand{\NUMBER}{10}
\newcommand{\EXERCISES}{3}
\newcommand{\DEADLINE}{01.02.2021}
\newcommand{\COURSE}{Math for Last-Layer Variational Inference}
\newcommand{\STUDENTA}{Philipp von Bachmann}
\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath, enumerate, amssymb, multirow, fancyhdr, color, graphicx, lastpage, listings, tikz, pdflscape, subfigure, float, polynom, hyperref, tabularx, forloop, geometry, listings, fancybox, tikz, forest, tabstackengine, cancel}
\input kvmacros
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
\pagestyle {fancy}
\fancyhead[C]{\COURSE}
\fancyhead[R]{\today}
\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{Page \thepage /\pageref*{LastPage}}
\def\header#1#2{
  \begin{center}
    {\Large Assignment #1}\\
    %{(Due by: #2)}
  \end{center}
}

\newcounter{punktelistectr}
\newcounter{punkte}
\newcommand{\punkteliste}[2]{%
  \setcounter{punkte}{#2}%
  \addtocounter{punkte}{-#1}%
  \stepcounter{punkte}%<-- also punkte = m-n+1 = Anzahl Spalten[1]
  \begin{center}%
  \begin{tabularx}{\linewidth}[]{@{}*{\thepunkte}{>{\centering\arraybackslash} X|}@{}>{\centering\arraybackslash}X}
      \forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
      {%
        \thepunktelistectr &
      }
      #2 &  $\Sigma$ \\
      \hline
      \forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
      {%
        &
      } &\\
      \forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
      {%
        &
      } &\\
    \end{tabularx}
  \end{center}
}
\begin{document}

\section{General Variational Inference}
    The general form of the ELBO is:
    \begin{align*}
        & \mathbb{E}_{q(\theta)}[\sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))}] - D_{KL}(q(\theta) \Vert p(\theta))\\
    \end{align*}

  \subsection{Likelihood for categorical data}
      Assuming $f_w(x)$ outputs a probability distribution over the classes, we use a categorial distribution for the likelihood
      \begin{align*}
          p(y_i \Vert f_w(x_i))
          &= \prod_{j=1}^k f_w(x_{i_j})^{y_{i_j}} \\
      \end{align*}
      If we have just one true class $l$ and one-hot encoded, we have:
      \begin{align*}
          = f_w(x_{i_l})
      \end{align*}

    \subsection{Likelihood for Regression}
      We have that $p(y \vert f_{\theta}(x)) \sim \mathcal{N}(y,
      f_{\theta}(x), \alpha^2)$. The log-likelihood of a normal is:
      \begin{align*}
        \sum_i \log{\mathcal{N}(y_i,f_{\theta}(x_i), \alpha^2)}
        &= \sum_i -\frac{1}{2}\log{2\pi \alpha} - \frac{1}{2\alpha^2} (y- f_{\theta}(x))^2\\
        &= -\frac{n}{2}\log{2\pi \alpha} - \frac{1}{2\alpha^2} \sum_i (y- f_{\theta}(x))^2
      \end{align*}
      For the multivariate case:
      \begin{align*}
        \log{\mathcal{N}(y,f_{\theta}(x), \Sigma)}
        &= -\frac{k}{2}\log{2\pi} - \frac{1}{2} \log(\vert \Sigma \vert) - \frac{1}{2}(y - f_{\theta}(x))^T \Sigma (y - f_{\theta}(x))
      \end{align*}

      \subsubsection{Closed form}
        \begin{align*}
          \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A)}]
          &\propto \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [(\phi_{X}\theta - y)^T A (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{\phi_X \theta \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [(\phi_{X}\theta - y)^T A (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{\phi_X \theta -y \sim N(\phi_X \mu - y, \phi_X^T \Sigma \phi_X + \alpha)} [(\phi_{X}\theta - y)^T A (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{f\sim N(\phi_X \mu - y, \phi_X^T \Sigma \phi_X + \alpha)} [f^T A f]\\
          &= Tr(A (\phi_X^T \Sigma \phi_X + \alpha)) + (\phi_X \mu - y)^T A (\phi_X \mu - y)
        \end{align*}

        \begin{align*}
          \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A^{-1})}]
          &\propto \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [(\phi_{X}\theta - y)^T A^{-1} (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{\phi_X \theta \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [(\phi_{X}\theta - y)^T A^{-1} (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{f_X \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [(f_X- y)^T A^{-1} (f_X- y)]\\
          &= \mathbb{E}_{f_X \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [f_X^T A^{-1} f_X - 2 f_X^T A^{-1} y + y^T A^{-1} y]\\
          &= \mathbb{E}_{f_X \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [f_X^T A^{-1} f_X]- 2 \phi_X \mu A^{-1} y + y^T A^{-1} y\\
          &= Tr(A^{-1} \phi_X^T \Sigma \phi_X) + (\phi_X \mu)^T A^{-1} (\phi_X \mu) - 2 \phi_X \mu A^{-1} y + y^T A^{-1} y\\
        \end{align*}
        

  \section{Predictions}
  \begin{align*}
      p(y \vert f_w(x)) 
      &= \mathbb{E}_{p(w)}[p(y \vert f_w(x))]\\
      &\approx E_{q(w)}[p(y \vert f_w(x))]\\
      &\approx \frac{1}{M}\sum_{i=1}^m P(y \vert f_{w_i}(x))
  \end{align*}

  \subsection{ELBO scaling}
    \subsubsection{Batch size}
      If we have $B$ equally sized batches with size $b$, then we need to scale the ELBO by
      \begin{align*}
        & \mathbb{E}_{q(\theta)}[\sum_{i=1}^b p(y_i \vert f(x_i))] - \frac{1}{B} D_{KL}(q(\theta) \Vert p(\theta))\\
      \end{align*}
      for each parameter update according to: "Practical Variational Inference
      for Neural Networks"
      % https://papers.nips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf
      or "Bayesian Learning via Stochastic Gradient Langevin Dynamics" (if we assume equally sized batches)
      % https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf

    \subsubsection{Mean instead of sum}
      If we use a sum for our likelihood-loss, the values could get too big,
      that's why we need to scale the KL div also by the batch size $b$ which results in
      \begin{align*}
        & \mathbb{E}_{q(\theta)}[\frac{1}{b}\sum_{i=1}^b p(y_i \vert f(x_i))] - \frac{1}{B *b} D_{KL}(q(\theta) \Vert p(\theta))\\
        &= \mathbb{E}_{q(\theta)}[\frac{1}{b}\sum_{i=1}^b p(y_i \vert f(x_i))] - \frac{1}{N} D_{KL}(q(\theta) \Vert p(\theta))\\
      \end{align*}
      where $N$ is the total number of samples-


\section{Different Kernels for Gaussians}
  \subsection{KL-Divergence Gaussian}
    The KL-Div between two Gaussians is:
    \begin{align*}
      D_{KL}(q(\theta) \Vert p(\theta))
      %&= \int_{- \infty}^\infty q(\theta) log(\frac{p(\theta)}{q(\theta)}) d\theta\\
      &= \mathbb{E}_{q(\theta)} [log(\frac{p(\theta)}{q(\theta)})] \\
      &= \mathbb{E}_{q(\theta)} [log(\frac{1}{\sqrt{(2 \pi)^{\frac{n}{2}} det(\Sigma_q)}} exp(-\frac{1}{2} (\theta - \mu_q)^T \Sigma^{-1}_q (\theta - \mu_q)))\\
      &- log(\frac{1}{\sqrt{(2 \pi)^{\frac{n}{2}} det(\Sigma_p)}} exp(-\frac{1}{2} (\theta - \mu_p)^T \Sigma^{-1}_p (\theta - \mu_p)))] \\
      &= \mathbb{E}_{q(\theta)} [log(\frac{1}{\sqrt{(2 \pi)^{\frac{n}{2}} det(\Sigma_q)}}) -\frac{1}{2} (\theta - \mu_q)^T \Sigma^{-1}_q (\theta - \mu_q)\\
      &- log(\frac{1}{\sqrt{(2 \pi)^{\frac{n}{2}} det(\Sigma_p)}}) + \frac{1}{2} (\theta - \mu_p)^T \Sigma^{-1}_p (\theta - \mu_p)] \\
      &= \mathbb{E}_{q(\theta)} [\frac{1}{2}log(\frac{det(\Sigma_p)}{det(\Sigma_q)}) -\frac{1}{2} (\theta - \mu_q)^T \Sigma^{-1}_q (\theta - \mu_q)\\
      & + \frac{1}{2} (\theta - \mu_p)^T \Sigma^{-1}_p (\theta - \mu_p)] \\
      % see for example:
      % http://stanford.edu/~jduchi/projects/general_notes.pdf
      % https://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians
      &= \frac{1}{2}(log(\frac{det(\Sigma_p)}{det(\Sigma_q)}) -n + tr(\Sigma^{-1}_p \Sigma_q ) + (\mu_p - \mu_q)^T \Sigma^{-1}_p (\mu_p - \mu_q))\\
  \end{align*}

  \subsection{Diagonal}
      We set the prior $p(\theta)$ as well as $q$ to a diagonal gaussian, this
      means the Kl-Divergence can be further simplified to:
      \begin{align}
          % &= \mathbb{E}_{q(\theta)} [\frac{1}{2}log(\prod_i\frac{ \Sigma_{p_i}}{\Sigma_{q_i}}) + \frac{1}{2} \sum_i - (w_{l_i} - \mu_{q_i})^2 \Sigma^{-1}_{q_i} + (w_{l_i} - \mu_{p_i})^2 \Sigma^{-1}_{p_i}]\\
          &= \frac{1}{2} \sum_{i=1}^n log(\frac{ \Sigma_{p_i}}{\Sigma_{q_i}}) - n + \frac{\Sigma_{q_i}}{\Sigma_{p_i}}  +  (\mu_{p_i} - \mu_{q_i})^2 \Sigma^{-1}_{p_i}
      \end{align}

      If we use a prior of $\mu_p=0$ and $\Sigma_p=Id$, then
      \begin{align}
          &= \frac{1}{2} \sum_{i=1}^n - log(\Sigma_{q_i}) - n + \Sigma_{q_i} +  \mu_{q_i}^2
      \end{align}
          
      % later in pytroch use:
      %torch.distributions.kl.kl_divergence
      % https://discuss.pytorch.org/t/use-kl-divergence-as-loss-between-two-multivariate-gaussians/40865

  \subsection{K-Fac}
      First K-Fac just has covariances within each layer, however as we have
      just one layer we have a full covariance matrix.
      Next K-Fac approximates each block covariance $C$ with a Kronecker-Product of two Matrices.
      $C= A \otimes B$

      \subsubsection{Determinant}
        If $A, n\times n$ and $B, m \times m$ are square, the determinant of a the Kronecker Product is
        \begin{align}\label{eq:KronDet}
          det(A \otimes B) = det(A)^n det(B)^m
        \end{align}
        
      \subsubsection{Cholesky decomposition}
        Because the covariance $C$ needs to be psd, each factor $A,B$ needs to be
        psd according to \ref{eq:KronDet}. We can achieve this by parametrizing them with a Cholesky
        decomposition. To do that, decompose $A,B$ in lower triangular matrices
        $L_A, L_B$ with positive diagonal elements, such that: $A=L_AL_A^T$,
        $B=L_BL_B^T$.
        Moreover, if we have the Choleksy decomposition then we can rewrite the Kronecker product as:
        \begin{align}\label{eq:CholKron}
          A \otimes B = L_AL_A^T \otimes L_BL_B^T = (L_A \otimes L_B) (L_A \otimes L_B)^T
        \end{align}
        % see: http://www.math.uwaterloo.ca/~hwolkowi/henry/reports/kronthesisschaecke04.pdf , page 8
        
      \subsubsection{Determinant together with Cholesky}
          The logdeterminant for the KL divergence with \ref{eq:KronDet} is:
          \begin{align}\label{eq:CholDet}
            \log \det (A \otimes B)
            &= \log (\det (A)^n \det (B)^m)\\
            &= n \cdot \log (\det (A)) + m \cdot \log ( \det (B)^m)\\
            &= 2n \cdot \sum_i \log (A_{ii}) + 2m \cdot \sum_i \log (B_{ii})
          \end{align}
          where we used the fact that $\det(L) = \prod_i L_{ii}$ for triangular matrices.

          However with equation \ref{eq:CholKron} we can simplify it easier:
          \begin{align}
            \log \det (A \otimes B)
            &= \log \det (L_AL_A^T \otimes L_BL_B^T)\\
            &= \log \det ((L_A \otimes L_B) (L_A \otimes L_B)^T)\\
            &= \log (\det (L_A \otimes L_B) \det((L_A \otimes L_B)^T)))\\
            &= \log ((\det (L_A \otimes L_B))^2)\\
            &= 2\log (\det (L_A \otimes L_B))\\
            &= 2 \sum_i \log (L_A \otimes L_B)_{ii}\\
          \end{align}
        
        
        
  
        % \subsubsection{Inverse}
        %   The Inverse if a Kronecker product is the Kronkecker-Product of the inverse:
        %   \begin{align}
        %     (A \otimes B)^{-1} = A^{-1} \otimes B^{-1}
        %   \end{align}
          
        %   for pytorch see: https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html

\section{Expectation}
In the ELBO, we sample from:
$\mathbb{E}_{q(\theta)}[\sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))}]$,
so with MC, we need to sample the weights from $\mathcal{N}(\theta, \mu, \Sigma)$.
However during test time, we sample from ${E}_{q(\theta)} [softmax(\Phi(X)\theta)]$
or $softmax(\Phi(X) E_{q(\theta)}[\theta])$.



\section{Single Sample ELBO}
  The general form of the ELBO is:
  \begin{align*}
      \mathbb{E}_{q(\theta)}[\sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))}] - D_{KL}(q(\theta) \Vert p(\theta))\\
  \end{align*}
  if we have just one sample the expected log likelihood collapses to a standard likelihood
  \begin{align*}
    \sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))} - D_{KL}(q(\theta) \Vert p(\theta))\\
  \end{align*}
  with added Noise for the last-layer weights during training

  \subsection{Regression}
    Assume we have a probability distribution $\theta \sim \mathcal{N}(\mu,
    \Sigma)$.
     Thus the forward pass $X\theta$ for a single sample becomes $X
    (\mu + \Sigma \epsilon)$, so we can rewrite the training objective as
    $\langle X (\mu + \Sigma \epsilon) - Y, X (\mu + \Sigma \epsilon) - Y\rangle$ and the derivative becomes
    \begin{align*}
      &\frac{\partial}{\partial \mu}\langle X (\mu + \Sigma \epsilon) - Y, X (\mu + \Sigma \epsilon) -Y \rangle\\
      &= \frac{\partial}{\partial \mu}\langle X \mu + X\Sigma \epsilon - Y, X \mu + X\Sigma \epsilon -Y \rangle\\
      &= 2X^T(X \mu  - Y) + 2 X^T X \Sigma \epsilon
    \end{align*}
    Which is the derivative of the normal training loss minus the std deviation
    of the weights scaled by the covariance of the dimensions
    The closed form solution would be:
    \begin{align*}
      0 &= 2X^T(X \mu  - Y) + 2 X^T X \Sigma \epsilon\\
      \mu &= (X^T X)^{-1} X^T Y + \Sigma \epsilon
    \end{align*}

    Observations
    \begin{itemize}
      \item For the perfect solution we would just need to offset by the drawn $\epsilon$
      \item $\epsilon$ could be seen as an per iteration noise to the gradient
      \item However $\epsilon$ changes every iteration, so to get best $\Sigma$ should be as small as possible
      \item if in contrast, we would take a large number of samples, $\Sigma$ shouldn't matter at all, because expected $\epsilon$ is 0
      \item maybe compare to paper 'Stochastic Gradient Descent as Approximate Bayesian Inference'
    \end{itemize}

    
    For $\Sigma$:
    \begin{align*}
      &\frac{\partial}{\partial \Sigma}\langle X (\mu + \Sigma \epsilon) - Y, X (\mu + \Sigma \epsilon) -Y \rangle\\
      &= 2X^T(X \mu  - Y) \epsilon^T + 2 X^T X \Sigma \epsilon \epsilon^T\\
      \Rightarrow \text{set to 0:}\\
      &0 = 2X^T(X \mu  - Y) \epsilon^T + 2 X^T X \Sigma \epsilon \epsilon^T\\
      - X^T X \mu \epsilon^T +  X^T Y \epsilon^T &= X^T X \Sigma \epsilon \epsilon^T\\
      \mu \epsilon^T + (X^T X)^{-1} X^T Y &= \Sigma \epsilon \epsilon^T\\
      \mu \epsilon^T (\epsilon \epsilon^T)^{-1} + (X^T X)^{-1} X^T Y (\epsilon \epsilon^T)^{-1}&= \Sigma\\
    \end{align*}



\end{document}

