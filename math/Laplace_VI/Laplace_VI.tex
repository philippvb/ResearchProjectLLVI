\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{pgfplots}
\usepackage{amsmath, enumerate, amssymb, multirow, fancyhdr, color, graphicx, lastpage, listings, tikz, pdflscape, subfigure, float, polynom, hyperref, tabularx, forloop, geometry, listings, fancybox, tikz, forest, tabstackengine, cancel}
\input kvmacros
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
\pagestyle {fancy}


\begin{document}


\title{VI vs Laplace}
\maketitle

\section{Mathematical derivation}
  \subsection{Natural Gradient for Gaussian distributions}

    Assume we have a loss function $L(\lambda)$ which depends on the natural
    parameters $\lambda$ of a distribution $q$, which is Gaussian with natural
    parameters and mean parameters:
    \begin{align*}
        \lambda_1 = \Sigma^{-1} \mu\\
        \lambda_2 = - \frac{1}{2} \Sigma^{-1}\\
        m_1 = \mu\\
        m_2 = \mu \mu^T + \Sigma
    \end{align*}
    It can be shown that in this case, the gradient wrt the mean parameters becomes,
    % note that we can express it again in terms of the normal gradient wrt mean parameters
    \begin{align*}
        \nabla_{m_1} L = \nabla_\mu L - 2 [\nabla_\Sigma L]\\
        \nabla_{m_2} L = \nabla_\Sigma L
    \end{align*}
    If we perform natural gradient VI in natural parameter space, we get the update rule:
    \begin{align*}
      \lambda_{t+1} = \lambda_t + \rho_t \nabla_m L(m_t)
    \end{align*}
    which says, that natural gradient in natural parameter space can be expressed using
    a derivative for the mean parameters. Thus, we can plug our gaussian natural
    parameters in and rearrange for mean and precision, which leads to the
    following update
    \begin{align*}
        \Sigma_{t+1}^{-1} = \Sigma^{-1}_t - 2 \rho [\nabla_\Sigma L_t]\\
        \mu_{t+1} = \mu_t + \rho \Sigma_{t+1} [\nabla_\mu L_t]
    \end{align*}

  \subsection{Plugging in the ELBO}\label{sec:ELBO}
    We can express the ELBO by: 
    \begin{align*}
      L(\mu, \Sigma) = \mathbb{E}_q[-N f(\theta) + \log(p(\theta)) - \log(q(\theta))]
    \end{align*}
    By Bonnets and Price,
    \begin{align*}
      \nabla_\mu \mathbb{E}_q[f(\theta)] = \mathbb{E}_q[g(\theta)]\\
      \nabla_\Sigma \mathbb{E}_q[f(\theta)] = \mathbb{E}_q[H(\theta)]\\
    \end{align*}
    where $g(\theta) = \nabla_\theta f(\theta)$ and $H(\theta) = \nabla^2_{\theta\theta} f(\theta)$.\\
    We can rewrite the gradients for the update of the mean and variance and
    thus plug in to above ($\alpha$ arises from prior):
    % see fast appendix d
    \begin{align*}
      \Sigma_{t+1}^{-1} = (1-\rho)\Sigma^{-1}_t + \rho \mathbb{E}_{q_t}[N H(\theta_t) + \alpha I]\\
      \mu_{t+1} = \mu_t - \rho \Sigma_{t+1}\mathbb{E}_{q_t}[N g(\theta_t) + \alpha \mu_t]
    \end{align*}

  In the paper Bayesian Learning Rule, they include the prior in the loss
  function as $\bar{l} = f(\theta) + \log{p(\theta)}$ and therefore get rid of the
  alpha terms:
  \begin{align*}
    \Sigma_{t+1}^{-1} = (1-\rho)\Sigma^{-1}_t + \rho \mathbb{E}_{q_t}[\nabla^2_{\theta\theta} \bar{l}(\theta)]\\
    \mu_{t+1} = \mu_t - \rho \Sigma_{t+1}\mathbb{E}_{q_t}[\nabla_{\theta} \bar{l}(\theta)]
  \end{align*}

  \subsection{Making Laplace and VI equal}

    \subsubsection{Expectation}
      In our update equations, we see that we need to compute the expectation
      over a gradient. 
      \begin{itemize}
        \item We could get to frequentist training by applying the
        delta method.
        \item If we train VI with single-sample, we would just need a weight pertubation in Laplace
      \end{itemize}

    \subsubsection{Online-Computation of the Hessian}
      Problem: We don't want to use online computation of Hessian for Laplace,
      but rather compute it afterwards at $\theta_*$\\
      Solution: We converge to an unbiased estimate of the $h(\theta_*)$ if we
      train long enough.

    \subsubsection{Newton optimization in frequentist training}
      In frequentist training, we would need to use Netwon-updates for optimization.
      However if we use momentum in VI, it can be shown that it is the same as
      Adam on the original objective plus an added weight pertubation, which we
      could get rid of with the delta method.



\section{Examples}

\subsection{Last-Layer Regression}
  We treat earlier layers as feature extractor of the features $\phi_X$ and
  since we optimizer it the standard way in VI as well, disregard them. We have
  the log-likelihood $f(\theta) = - \frac{1}{N} \sum_i \log(p(y_i \vert x_i,
  \theta))$ and assume our prior is $p(\theta) = \mathcal{N}(0, 1)$ which is
  regular weight-decay up to a constant.

  \subsubsection{Frequentist approach}
    Our Loss function with L2 Reguralization is (where $A$ is data noise):
    \begin{align*}
      L(\theta) &= f(\theta) + \alpha \cdot \theta^2 \\
      &= \frac{k}{2}\log{2\pi} + \frac{1}{2} \log(\vert A^{-1} \vert) + (\phi_{X}\theta - y)^T A^{-1} (\phi_{X}\theta - y) + \alpha \cdot \theta^2\\
    \end{align*}

    taking the first and second order derivative wrt $\theta$:
    \begin{align*}
      \nabla_\theta L(\theta) &= 2 \phi_X^T A (\phi_{X} \theta - y) + 2 \alpha \theta\\
      \nabla^2_{\theta\theta} L(\theta) &= 2 \phi_X^T A \phi_{X} + 2 \alpha\\
    \end{align*}

    so if we do Newton steps our update would look like:
    \begin{align*}
      \theta_{t+1} &= \theta_t - \rho \cdot (2 \phi_X^T A \phi_{X} + 2 \alpha)^{-1} \cdot (2 \phi_X^T A (\phi_{X} \theta - y) + 2 \alpha \theta)\\
    \end{align*}

    After $n$ steps our laplace approximation would become:
    \begin{align*}
      q(\theta) = \mathcal{N}(\theta_n, 2 \phi_X^T A \phi_{X} + 2 \alpha)
    \end{align*}

  \subsubsection{VI}
    Even though our objective function is the ELBO, we have seen in section
    \ref{sec:ELBO} that we can derive easier update rules.

    \begin{align*}
      \Sigma_{t+1}^{-1} = (1-\rho)\Sigma^{-1}_t + \rho \mathbb{E}_{q_t}[\nabla^2_{\theta\theta} \bar{l}(\theta)]\\
      \mu_{t+1} = \mu_t - \rho \Sigma_{t+1}\mathbb{E}_{q_t}[\nabla_{\theta} \bar{l}(\theta)]
    \end{align*}

    As we have already computed the necessary derivatives before, we can just plug them in:
    \begin{align*}
      \Sigma_{t+1}^{-1}
      &= (1-\rho)\Sigma^{-1}_t + \rho \mathbb{E}_{q_t}[2 \phi_X^T A \phi_{X} + 2 \alpha]\\
      &= (1-\rho)\Sigma^{-1}_t + \rho (2 \phi_X^T A \phi_{X} + 2 \alpha)\\
      &= 2 \phi_X^T A \phi_{X} + 2 \alpha\\
    \end{align*}
    where the last step follows since the second order derivative is constant.
    \begin{align*}
      \mu_{t+1} 
      &= \mu_t - \rho \Sigma_{t+1}\mathbb{E}_{q_t}[\nabla_{\theta} \bar{l}(\theta)]\\
      &= \mu_t - \rho \Sigma_{t+1}\mathbb{E}_{q_t}[2 \phi_X^T A (\phi_{X} \theta - y) + 2 \alpha \theta]\\
      &= \mu_t - \rho \Sigma_{t+1} (2 \phi_X^T A (\phi_{X} \mu - y) + 2 \alpha \mu)\\
      &= \mu_t - \rho (2 \phi_X^T A \phi_{X} + 2 \alpha)^{-1} (2 \phi_X^T A (\phi_{X} \mu - y) + 2 \alpha \mu)\\
    \end{align*}
    Now we have the same update for both cases.

    Normally, step three follows from a delta approximation, however in this casem, it is exact.
      
\section{Evidence in Laplace is same as ELBO}

Here, we show a new way in which the ELBO arises from the Laplace
approximation. We start by trying to approximate $p(\theta \vert D)
= \frac{1}{Z}h(\theta)$ by a distribution $q(\theta \vert D)$ over
our parameters after training with Loss $L(\theta, D) \propto
h(\theta)$. We can show that in the case of a second-order Talor
expansion of $h(\theta)$, this estimation is given by a Gaussian:
$q(\theta \vert D) = \mathcal{N}(\theta_{MAP}, -(\nabla^2_\theta
L(\theta)\vert_{\theta_{MAP}})^{-1})$. The normalization constant of
this Gaussian is therefore $p(D) = Z =
\exp(-L(\theta_{MAP}))(2\pi)^{\frac{D}{2}}
\det(\Sigma)^{\frac{1}{2}}$. Taking the logarithm yields $\log(Z) =
-L(\theta_{MAP}) + H(q(\theta \vert D))$, which is also known as the
ELBO. NOTE: constant is missing. Therefore, the ELBO can be seen as a second-order Taylor
expansion together with the delta method, given we are at
$\theta_{MAP}$.


In other words, by assuming a Gaussian distribution, we loose all
higher than second-order terms to the KL-Divergence:
\begin{align*}
    log(p(x)) = D_{KL}(q(\theta \vert D) \vert p(\theta \vert D)) + ELBO
\end{align*}
Here we capture all up to second-order terms of the true posterior
by choosing a normal distribution and fail to approximate all higher
order terms whose loss is then captured in the KL divergence..

\subsection{Where does that relationship come from}
    The first part jut arises from the delta approximation. The
    second part comes that in case of Natural-Gradient VI, the
    variance is given by the hessian, thus the second order part of
    the taylor expansion.


\subsection{A different view}
We saw already that second-order taylor approximating $h(\theta)$
yields the Laplace approximation, which is the same as VI for
gaussians. We can also see this connection by noting that both
Laplace and VI approximate the evidence in the same way. This
happens because we see that the VI only contains second-order terms,
which means that a second-order taylor will capture everything.


\subsection{Can we train with the marginal likelihood in Laplace}
The log marginal likelihood is given by:
\begin{align*}
    \log Z(\theta) =-L(\theta_{MAP}))+ \frac{D}{2}\log(2\pi) + \frac{1}{2}\log\det(\Sigma)
\end{align*}
Therefore, our gradient 
\begin{align}
    \nabla_\theta log(Z) 
    &= ... + \frac{1}{2} \frac{1}{det(\Sigma)}\cdot TR(\Sigma^{-1} \frac{\partial \Sigma}{\partial x})\\
    &= ... + \frac{1}{2} \frac{1}{det(-(\nabla^2_\theta L(\theta)^{-1})}\cdot TR(\nabla^2_\theta L(\theta) \cdot  \nabla^3_\theta L(\theta))\\
\end{align}

Which shouldn't be the same as the ELBO updates. Problem: Normally
we set $\Sigma$ to the Hessian but we never propagate it through. 

\subsection{Last-Layer Marginal Likelihood for Regression}
  In case of Regression, we have the closed form second-order derivative:
  \begin{align}
    \nabla^2_\theta \log p(y, \theta \vert x) &= \phi_X^T \phi_X A^{-1^T} + \log p(\theta)\\
    &= - \phi_X^T \phi_X A^{-1^T} + 1\\
  \end{align}
  see also bishop 3.81.

  The problem is now that our next derivative becomes 0 wrt the ll weights, thus
  it isn't included in the gradient.

  However, we can take use the Fisher/GGN instead:
  \begin{align}
    &E_{p(y \vert x, \theta)}[\nabla_\theta \log(p(y \vert x, \theta)) \cdot \nabla_\theta \log(p(y \vert x, \theta))^T]\\
    &= E_{p(y \vert x, \theta)}[\Phi_X^T (f(x, \theta) - y) \cdot (\Phi_X^T (f(x, \theta) - y))^T]\\
    &= E_{p(y \vert x, \theta)}[\Phi_X^T (f(x, \theta) - y) \cdot (f(x, \theta) - y) \Phi_X ]\\
    &= E_{p(y \vert x, \theta)}[\Phi_X^T (f(x, \theta) - y)^2 \Phi_X ]\\
  \end{align}


  \subsection{Sigmoid classification}
    see also Bishop, 4.97:
    \begin{align}
      \nabla^2_\theta -\log p(y \vert \theta, x) 
      &= \nabla^2_\theta - \sum_i y_i \log(f(x_i, \theta)) + (1 - y_i) \log(1-f(x_i, \theta))\\
      &= \nabla_\theta \sum_i (f(x_i, \theta)- y_i) \phi_{X_i}\\
      &= \sum_i \frac{\partial f(x_i, \theta)}{ \partial \theta} \phi_{X_i}\\
      &= \sum_i f(x_i, \theta) \cdot (1 - f(x_i, \theta)) \cdot \phi_{X_i}^2\\
    \end{align}
    according to Bishop 4.91
    \begin{align}
      \nabla_\theta f(x_i, \theta) 
      &= \nabla_\theta \sigma(\phi_{X_i} \theta)\\
      &= \sigma(\phi_{X_i} \theta) \cdot (1 - \sigma(\phi_{X_i} \theta)) \cdot \nabla_\theta \phi_{X_i} \theta \\
      &= \sigma(\phi_{X_i} \theta) \cdot (1 - \sigma(\phi_{X_i} \theta)) \cdot \phi_{X_i}\\
      &= f(x_i, \theta) \cdot (1 - f(x_i, \theta)) \cdot \phi_{X_i}\\
    \end{align}





\section{Post-hoc Laplace is nearly as good as post-hopc VI}
The main point is that at $\theta_{MAP}$, Laplace approximation is the same
as continuing to train with Variational Inference. Thus, the question is not
if Laplace is a good method, but rather how expensive we want to be during
training.

My main point is that we shouldn't choose frequentist training because we
have Laplace to save us, but if we have other constrains why VI is
infeasible (eg computationally), then Laplace is a good way to still get a
approximation for a weight distribution.

\subsection{Laplace is the same as VI at $\theta_{MAP}$}

    Suppose we train a neural network with the loss function
    \begin{align}
        L(\theta) = h(\theta) = \log(p(D\vert \theta) p(\theta))
    \end{align}
    which is standard $MAP$ inference for DNNs. At the end of training, we
    arrive at a point $\theta_{MAP}$. Now we want to employ a distribution
    $q(\theta)$ over the weights of the network. Two usual choices for
    finding a MAP distribution are VI and Laplace, we show that those are
    equal:

    From multiple papers, we get the update equations for the mean and variance:
    \begin{align*}
        \Sigma_{t+1}^{-1} = (1-\rho)\Sigma^{-1}_t + \rho \mathbb{E}_{q_t}[\nabla^2_{\theta\theta} h(\theta)]\\
        \mu_{t+1} = \mu_t - \rho \Sigma_{t+1}\mathbb{E}_{q_t}[\nabla_{\theta} h(\theta)]
        \end{align*}
    As we are at $\theta_{MAP}$, we know that $\nabla_{\theta} h(\theta)
    = 0$. This means that $\mu_{t+1} = \mu_t$ and it follows for the variance, that $\Sigma_{n}^{-1} = \mathbb{E}_{q_t}[\nabla^2_{\theta\theta} h(\theta)]$.

    In Laplace, we have the approximation $\mu = \theta_{MAP}$ and $\Sigma^{-1} = \nabla^2_{\theta\theta} h(\theta) \vert_{\theta_{MAP}}$.

    To unite the two, we need to take a first-order delta approximation $\mathbb{E}_{q_t}[\nabla^2_{\theta\theta} h(\theta)] =\nabla^2_{\theta\theta} h(\theta) \vert_{\theta_{MAP}}$.

    A note for the delta approximation: I think delta is still a crucial
    part here, since this means that even if we are close to a point of high
    loss, we wouldn't move on. Therefore, I think Laplace shines when we
    don't want to optimize further but rather just want the best guess at
    the moment.


\section{Difference between VI and frequentist training}
  This section highlights the key differences between training a Network
  with Variational Inference and standard frequentist training. The main
  result is that Natural Gradient Variational Inference can be seen as frequentist training
  where we take Netwon steps.

  \subsection{First versus second-order gradients}
      We start with the update equation for Variational Inference:
      \begin{align*}
          \Sigma_{t+1}^{-1} = (1-\rho)\Sigma^{-1}_t + \rho \mathbb{E}_{q_t}[\nabla^2_{\theta\theta} h(\theta)]\\
          \mu_{t+1} = \mu_t - \rho \Sigma_{t+1}\mathbb{E}_{q_t}[\nabla_{\theta} h(\theta)]
      \end{align*}
      We see that we scale the mean update with the inverse Hessian. Thus,
      our mean update resembles a Netwon step. We also note that if we
      would do Newton in our frequentist approach, we would have all the
      necessary quantities to do VI, except the expectation over the
      Hessian.
  \subsection{Approximating the Expectation}    
      We saw that one key difference is the expectation over the gradient
      and Hessian during VI against the normal Hessian in Newton steps.
      Here we argue, that the expectation adds an implicit reguralization
      effect.

      \subsubsection{The delta method}
          We first note that we can unite both approaches by doing a
          first-oder delta approximation as above. However, the advantage the
          expectation is that our algorithm will stay away from areas where we
          are close to high loss. This adds an implicit reguralizer.

      \subsubsection{Single-Sample ELBO}
          The problem of computing the expectation also arises in the case
          of VI. Here, we often deal with that by approximating the
          expectation with a Single-Sample. If we do that, the update becomes:
          \begin{align*}
              \mu_{t+1} = \mu_t - \rho \Sigma_{t+1}\nabla_{\theta} h(\theta)
          \end{align*}
          where $\theta \sim Q$. This gives rise to multiple frequentist VI ideas:
          \begin{itemize}
              \item VADAM: Here, we estimate our Variance like in Adam and
              then one sample during training.
          \end{itemize}

\section{Dumb ideas}
  \subsection{Single-Sample VI wit Laplace}
    For training, we use the update from single sample ELBO, so we do a Netwon step update. However only
    every $d$ Epochs, we update our guess from the Variance by Laplace.

  \subsection{Don't use delta}
    The delta approximation is quite bad, also when we approximate it at
    $\theta_{MAP}$. However, what would happen if for approximation of the
    Hessian at $\theta_{MAP}$, we dont use delta but rather MC sample where we
    set the distribution to Laplaces guess.
    
      


\end{document}

