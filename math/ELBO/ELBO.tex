\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{pgfplots}
\usepackage{amsmath, enumerate, amssymb, multirow, fancyhdr, color, graphicx, lastpage, listings, tikz, pdflscape, subfigure, float, polynom, hyperref, tabularx, forloop, geometry, listings, fancybox, tikz, forest, tabstackengine, cancel}
\input kvmacros
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
\pagestyle {fancy}


\begin{document}

\section{General Variational Inference}
    The general form of the ELBO is:
    \begin{align*}
        & \mathbb{E}_{q(\theta)}[\sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))}] - D_{KL}(q(\theta) \Vert p(\theta))\\
    \end{align*}

  \subsection{Likelihood for categorical data}
      Assuming $f_w(x)$ outputs a probability distribution over the classes, we use a categorial distribution for the likelihood
      \begin{align*}
          p(y_i \Vert f_w(x_i))
          &= \prod_{j=1}^k f_w(x_{i_j})^{y_{i_j}} \\
      \end{align*}
      If we have just one true class $l$ and one-hot encoded, we have:
      \begin{align*}
          = f_w(x_{i_l})
      \end{align*}

    \subsection{Likelihood for Regression}
      We have that $p(y \vert f_{\theta}(x)) \sim \mathcal{N}(y,
      f_{\theta}(x), \alpha^2)$. The log-likelihood of a normal is:
      \begin{align*}
        \sum_i \log{\mathcal{N}(y_i,f_{\theta}(x_i), \alpha^2)}
        &= \sum_i -\frac{1}{2}\log{2\pi \alpha} - \frac{1}{2\alpha^2} (y- f_{\theta}(x))^2\\
        &= -\frac{n}{2}\log{2\pi \alpha} - \frac{1}{2\alpha^2} \sum_i (y- f_{\theta}(x))^2
      \end{align*}
      For the multivariate case:
      \begin{align*}
        \log{\mathcal{N}(y,f_{\theta}(x), \Sigma)}
        &= -\frac{k}{2}\log{2\pi} - \frac{1}{2} \log(\vert \Sigma \vert) - \frac{1}{2}(y - f_{\theta}(x))^T \Sigma (y - f_{\theta}(x))
      \end{align*}

      \subsubsection{Closed form}
        \begin{align*}
          \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A)}]
          &\propto \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [(\phi_{X}\theta - y)^T A (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{\phi_X \theta \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [(\phi_{X}\theta - y)^T A (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{\phi_X \theta -y \sim N(\phi_X \mu - y, \phi_X^T \Sigma \phi_X + \alpha)} [(\phi_{X}\theta - y)^T A (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{f\sim N(\phi_X \mu - y, \phi_X^T \Sigma \phi_X + \alpha)} [f^T A f]\\
          &= Tr(A (\phi_X^T \Sigma \phi_X + \alpha)) + (\phi_X \mu - y)^T A (\phi_X \mu - y)
        \end{align*}

        \begin{align*}
          \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [\log{\mathcal{N}(y, \phi_{X}\theta, A^{-1})}]
          &\propto \mathbb{E}_{\theta \sim N(\mu, \Sigma)} [(\phi_{X}\theta - y)^T A^{-1} (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{\phi_X \theta \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [(\phi_{X}\theta - y)^T A^{-1} (\phi_{X}\theta - y)]\\
          &= \mathbb{E}_{f_X \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [(f_X- y)^T A^{-1} (f_X- y)]\\
          &= \mathbb{E}_{f_X \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [f_X^T A^{-1} f_X - 2 f_X^T A^{-1} y + y^T A^{-1} y]\\
          &= \mathbb{E}_{f_X \sim N(\phi_X \mu, \phi_X^T \Sigma \phi_X)} [f_X^T A^{-1} f_X]- 2 \phi_X \mu A^{-1} y + y^T A^{-1} y\\
          &= Tr(A^{-1} \phi_X^T \Sigma \phi_X) + (\phi_X \mu)^T A^{-1} (\phi_X \mu) - 2 \phi_X \mu A^{-1} y + y^T A^{-1} y\\
        \end{align*}
        

  \section{Predictions}
  \begin{align*}
      p(y \vert f_w(x)) 
      &= \mathbb{E}_{p(w)}[p(y \vert f_w(x))]\\
      &\approx E_{q(w)}[p(y \vert f_w(x))]\\
      &\approx \frac{1}{M}\sum_{i=1}^m P(y \vert f_{w_i}(x))
  \end{align*}

  \subsection{ELBO scaling}
    \subsubsection{Batch size}
      If we have $B$ equally sized batches with size $b$, then we need to scale the ELBO by
      \begin{align*}
        & \mathbb{E}_{q(\theta)}[\sum_{i=1}^b p(y_i \vert f(x_i))] - \frac{1}{B} D_{KL}(q(\theta) \Vert p(\theta))\\
      \end{align*}
      for each parameter update according to: "Practical Variational Inference
      for Neural Networks"
      % https://papers.nips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf
      or "Bayesian Learning via Stochastic Gradient Langevin Dynamics" (if we assume equally sized batches)
      % https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf

    \subsubsection{Mean instead of sum}
      If we use a sum for our likelihood-loss, the values could get too big,
      that's why we need to scale the KL div also by the batch size $b$ which results in
      \begin{align*}
        & \mathbb{E}_{q(\theta)}[\frac{1}{b}\sum_{i=1}^b p(y_i \vert f(x_i))] - \frac{1}{B *b} D_{KL}(q(\theta) \Vert p(\theta))\\
        &= \mathbb{E}_{q(\theta)}[\frac{1}{b}\sum_{i=1}^b p(y_i \vert f(x_i))] - \frac{1}{N} D_{KL}(q(\theta) \Vert p(\theta))\\
      \end{align*}
      where $N$ is the total number of samples-


\section{Different Kernels for Gaussians}
  \subsection{KL-Divergence Gaussian}
    The KL-Div between two Gaussians is:
    \begin{align*}
      D_{KL}(q(\theta) \Vert p(\theta))
      %&= \int_{- \infty}^\infty q(\theta) log(\frac{p(\theta)}{q(\theta)}) d\theta\\
      &= \mathbb{E}_{q(\theta)} [log(\frac{p(\theta)}{q(\theta)})] \\
      &= \mathbb{E}_{q(\theta)} [log(\frac{1}{\sqrt{(2 \pi)^{\frac{n}{2}} det(\Sigma_q)}} exp(-\frac{1}{2} (\theta - \mu_q)^T \Sigma^{-1}_q (\theta - \mu_q)))\\
      &- log(\frac{1}{\sqrt{(2 \pi)^{\frac{n}{2}} det(\Sigma_p)}} exp(-\frac{1}{2} (\theta - \mu_p)^T \Sigma^{-1}_p (\theta - \mu_p)))] \\
      &= \mathbb{E}_{q(\theta)} [log(\frac{1}{\sqrt{(2 \pi)^{\frac{n}{2}} det(\Sigma_q)}}) -\frac{1}{2} (\theta - \mu_q)^T \Sigma^{-1}_q (\theta - \mu_q)\\
      &- log(\frac{1}{\sqrt{(2 \pi)^{\frac{n}{2}} det(\Sigma_p)}}) + \frac{1}{2} (\theta - \mu_p)^T \Sigma^{-1}_p (\theta - \mu_p)] \\
      &= \mathbb{E}_{q(\theta)} [\frac{1}{2}log(\frac{det(\Sigma_p)}{det(\Sigma_q)}) -\frac{1}{2} (\theta - \mu_q)^T \Sigma^{-1}_q (\theta - \mu_q)\\
      & + \frac{1}{2} (\theta - \mu_p)^T \Sigma^{-1}_p (\theta - \mu_p)] \\
      % see for example:
      % http://stanford.edu/~jduchi/projects/general_notes.pdf
      % https://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians
      &= \frac{1}{2}(log(\frac{det(\Sigma_p)}{det(\Sigma_q)}) -n + tr(\Sigma^{-1}_p \Sigma_q ) + (\mu_p - \mu_q)^T \Sigma^{-1}_p (\mu_p - \mu_q))\\
  \end{align*}

  \subsection{Diagonal}
      We set the prior $p(\theta)$ as well as $q$ to a diagonal gaussian, this
      means the Kl-Divergence can be further simplified to:
      \begin{align}
          % &= \mathbb{E}_{q(\theta)} [\frac{1}{2}log(\prod_i\frac{ \Sigma_{p_i}}{\Sigma_{q_i}}) + \frac{1}{2} \sum_i - (w_{l_i} - \mu_{q_i})^2 \Sigma^{-1}_{q_i} + (w_{l_i} - \mu_{p_i})^2 \Sigma^{-1}_{p_i}]\\
          &= \frac{1}{2} (- n + \sum_{i=1}^n log(\frac{ \Sigma_{p_i}}{\Sigma_{q_i}}) + \frac{\Sigma_{q_i}}{\Sigma_{p_i}}  +  (\mu_{p_i} - \mu_{q_i})^2 \Sigma^{-1}_{p_i})
      \end{align}

      If we use a prior of $\mu_p=0$ and $\Sigma_p=Id$, then
      \begin{align}
          &= \frac{1}{2} (- n + \sum_{i=1}^n - log(\Sigma_{q_i}) + \Sigma_{q_i} +  \mu_{q_i}^2)
      \end{align}
          
      % later in pytroch use:
      %torch.distributions.kl.kl_divergence
      % https://discuss.pytorch.org/t/use-kl-divergence-as-loss-between-two-multivariate-gaussians/40865

  \subsection{K-Fac}
      First K-Fac just has covariances within each layer, however as we have
      just one layer we have a full covariance matrix.
      Next K-Fac approximates each block covariance $C$ with a Kronecker-Product of two Matrices.
      $C= A \otimes B$

      \subsubsection{Determinant}
        If $A, n\times n$ and $B, m \times m$ are square, the determinant of a the Kronecker Product is
        \begin{align}\label{eq:KronDet}
          det(A \otimes B) = det(A)^n det(B)^m
        \end{align}
        
      \subsubsection{Cholesky decomposition}
        Because the covariance $C$ needs to be psd, each factor $A,B$ needs to be
        psd according to \ref{eq:KronDet}. We can achieve this by parametrizing them with a Cholesky
        decomposition. To do that, decompose $A,B$ in lower triangular matrices
        $L_A, L_B$ with positive diagonal elements, such that: $A=L_AL_A^T$,
        $B=L_BL_B^T$.
        Moreover, if we have the Choleksy decomposition then we can rewrite the Kronecker product as:
        \begin{align}\label{eq:CholKron}
          A \otimes B = L_AL_A^T \otimes L_BL_B^T = (L_A \otimes L_B) (L_A \otimes L_B)^T
        \end{align}
        % see: http://www.math.uwaterloo.ca/~hwolkowi/henry/reports/kronthesisschaecke04.pdf , page 8
        
      \subsubsection{Determinant together with Cholesky}
          The logdeterminant for the KL divergence with \ref{eq:KronDet} is:
          \begin{align}\label{eq:CholDet}
            \log \det (A \otimes B)
            &= \log (\det (A)^n \det (B)^m)\\
            &= n \cdot \log (\det (A)) + m \cdot \log ( \det (B)^m)\\
            &= 2n \cdot \sum_i \log (A_{ii}) + 2m \cdot \sum_i \log (B_{ii})
          \end{align}
          where we used the fact that $\det(L) = \prod_i L_{ii}$ for triangular matrices.

          However with equation \ref{eq:CholKron} we can simplify it easier:
          \begin{align}
            \log \det (A \otimes B)
            &= \log \det (L_AL_A^T \otimes L_BL_B^T)\\
            &= \log \det ((L_A \otimes L_B) (L_A \otimes L_B)^T)\\
            &= \log (\det (L_A \otimes L_B) \det((L_A \otimes L_B)^T)))\\
            &= \log ((\det (L_A \otimes L_B))^2)\\
            &= 2\log (\det (L_A \otimes L_B))\\
            &= 2 \sum_i \log (L_A \otimes L_B)_{ii}\\
          \end{align}
        
        
        
  
        % \subsubsection{Inverse}
        %   The Inverse if a Kronecker product is the Kronkecker-Product of the inverse:
        %   \begin{align}
        %     (A \otimes B)^{-1} = A^{-1} \otimes B^{-1}
        %   \end{align}
          
        %   for pytorch see: https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html

\section{Expectation}
In the ELBO, we sample from:
$\mathbb{E}_{q(\theta)}[\sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))}]$,
so with MC, we need to sample the weights from $\mathcal{N}(\theta, \mu, \Sigma)$.
However during test time, we sample from ${E}_{q(\theta)} [softmax(\Phi(X)\theta)]$
or $softmax(\Phi(X) E_{q(\theta)}[\theta])$.



\section{Single Sample ELBO}
  The general form of the ELBO is:
  \begin{align*}
      \mathbb{E}_{q(\theta)}[\sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))}] - D_{KL}(q(\theta) \Vert p(\theta))\\
  \end{align*}
  if we have just one sample the expected log likelihood collapses to a standard likelihood
  \begin{align*}
    \sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))} - D_{KL}(q(\theta) \Vert p(\theta))\\
  \end{align*}
  with added Noise for the last-layer weights during training

  \subsection{Regression}\label{sec:SingleRegression}
    Assume we have a probability distribution $\theta \sim \mathcal{N}(\mu,
    \Sigma)$.
     Thus the forward pass $X\theta$ for a single sample becomes $X
    (\mu + \Sigma \epsilon)$, so we can rewrite the training objective (assuming unit data variance) as
    $\langle X (\mu + \Sigma \epsilon) - Y, X (\mu + \Sigma \epsilon) - Y\rangle$ and the derivative becomes
    \begin{align*}
      &\frac{\partial}{\partial \mu}\langle X (\mu + \Sigma \epsilon) - Y, X (\mu + \Sigma \epsilon) -Y \rangle\\
      &= \frac{\partial}{\partial \mu}\langle X \mu + X\Sigma \epsilon - Y, X \mu + X\Sigma \epsilon -Y \rangle\\
      &= 2X^T(X \mu  - Y) + 2 X^T X \Sigma \epsilon
    \end{align*}
    Which is the derivative of the normal training loss minus the std deviation
    of the weights scaled by the covariance of the dimensions
    The closed form solution would be:
    \begin{align*}
      0 &= 2X^T(X \mu  - Y) + 2 X^T X \Sigma \epsilon\\
      \mu &= (X^T X)^{-1} X^T Y + \Sigma \epsilon
    \end{align*}

    Observations
    \begin{itemize}
      \item For the perfect solution we would just need to offset by the drawn $\epsilon$
      \item $\epsilon$ could be seen as an per iteration noise to the gradient
      \item However $\epsilon$ changes every iteration, so to get best $\Sigma$ should be as small as possible
      \item if in contrast, we would take a large number of samples, $\Sigma$ shouldn't matter at all, because expected $\epsilon$ is 0
      \item maybe compare to paper 'Stochastic Gradient Descent as Approximate Bayesian Inference'
    \end{itemize}

    
    For $\Sigma$:
    \begin{align*}
      &\frac{\partial}{\partial \Sigma}\langle X (\mu + \Sigma \epsilon) - Y, X (\mu + \Sigma \epsilon) -Y \rangle\\
      &= 2X^T(X \mu  - Y) \epsilon^T + 2 X^T X \Sigma \epsilon \epsilon^T\\
      \Rightarrow \text{set to 0:}\\
      &0 = 2X^T(X \mu  - Y) \epsilon^T + 2 X^T X \Sigma \epsilon \epsilon^T\\
      - X^T X \mu \epsilon^T +  X^T Y \epsilon^T &= X^T X \Sigma \epsilon \epsilon^T\\
      -\mu \epsilon^T + (X^T X)^{-1} X^T Y e^T &= \Sigma \epsilon \epsilon^T\\
      -\mu \epsilon^{-1} + (X^T X)^{-1} X^T Y \epsilon^{-1}&= \Sigma\\ % not sure if this line holds since epsilon has no inverse on itself
    \end{align*}

    So to solve we would:
    \begin{align}
      -\mu \epsilon^T + (X^T X)^{-1} X^T Y \epsilon^T &= \Sigma \epsilon \epsilon^T\\
      - ((X^T X)^{-1} X^T Y + \Sigma \epsilon) \epsilon^T + (X^T X)^{-1} X^T Y \epsilon^T &= \Sigma \epsilon \epsilon^T\\
      - (X^T X)^{-1} X^T Y \epsilon^T + (X^T X)^{-1} X^T Y \epsilon^T &= 2\Sigma \epsilon \epsilon^T\\
      0 &= 2\Sigma \epsilon \epsilon^T\\
    \end{align}


    \subsection{Connection to Laplace}
    Single Sample ELBO:
    \begin{align*}
      \sum_{i=1}^n \log{p(y_i \vert f_{\theta}(x_i))} - D_{KL}(q(\theta) \Vert p(\theta))\\
    \end{align*}
    for $\theta \sim \mathcal{N}(\mu,\Sigma)$.
    If we redefine this loss as a probability distribution, more specific a Gibbs distribution, we have:
    \begin{align*}
      \prod_{i=1}^n p(y_i \vert f_{\theta}(x_i)) \cdot \exp(- D_{KL}(q(\theta) \Vert p(\theta)))\\
    \end{align*}
    so if we redefine the KL div as a prior over the weights, we have:
    \begin{align*}
      \prod_{i=1}^n p(y_i \vert f_{\theta}(x_i)) \cdot p(\theta)\\
    \end{align*}
    \subsubsection{Mean}
      Maximizing wrt to $\theta$ gives $\theta_{MAP}$. This optimizes the same
      equation as the Laplace approximation, however here the weights are sampled
      from a distribution rather than taking the mean. To get to the same result as Laplace, we have two options:
      \begin{itemize}
        \item Assume the Variance of the weights is fixed at 0
        \item Assume that by taking multiple passes over the data, the variance
        just adds some noise to the weights which however will be 0 in
        expectation, thus performing a kind of noise SGD.
      \end{itemize}
    \subsubsection{Variance}
      The Laplace approximation for the Variance is:
      \begin{equation}
        \Sigma = (- \nabla^2 \log p(\theta \vert D) \vert_{\theta_{MAP}})^{-1} = (- \nabla^2 \log p(D \vert \theta) \vert_{\theta_{MAP}} - \log p(\theta))^{-1}
      \end{equation}
      % If we go to the regression case, we saw in \ref{sec:SingleRegression} that
      For the laplace approximation in case of regression, we need to compute:
      \begin{align}
        \nabla^2_{\mu} \log p(\theta \vert D)
        &\propto \nabla^2_{\mu} \langle X \mu - Y, A^{-1} (X \mu -Y) \rangle \\
        &= \nabla_{\mu} 2 X^T A^{-1} (X\mu - Y)\\
        &= 2 X^T A^{-1} X 
      \end{align} which is constant since independent of $\theta_{MAP}$. Now we take the inverse:
      $= \frac{1}{2}  X^{-1} A X^{T^{-1}}$

      For VI, we get the general derivative as, see section \ref{sec:SingleRegression} :
      \begin{align}
        -((X^T X)^{-1} X^T Y + \Sigma \epsilon) \epsilon^T (\epsilon \epsilon^T)^{-1} + (X^T X)^{-1} X^T Y (\epsilon \epsilon^T)^{-1}&= \Sigma\\
        2 (X^T X)^{-1} X^T Y (\epsilon \epsilon^T)^{-1} & = 2 \Sigma\\ % TODO: verify
         (X^T X)^{-1} X^T Y (\epsilon \epsilon^T)^{-1} &=  \Sigma
      \end{align}


    
    



    \subsection{KL Divergence}
    For Diagonal with zero mean prior and variance of 1, the kl divergence is
    \begin{align}
      \frac{1}{2}  (- n + \sum_{i=1}^n - \log(\Sigma_{q_i}) + \Sigma_{q_i} +  \mu_{q_i}^2)
    \end{align}
    So the last term is just normal weight decay, for the variance we can do a plot\\
    \begin{tikzpicture}
      \begin{axis}[xmin = 0, xmax = 5]
        \addplot[        
        domain = 0:30,
        samples = 200,
        smooth,
        thick,
        blue,] {-ln(x)+x};
      \end{axis}
    \end{tikzpicture}



\end{document}

